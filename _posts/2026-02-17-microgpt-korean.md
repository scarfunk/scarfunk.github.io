---
layout: post
title: "[ë²ˆì—­] microgpt - 200ì¤„ì˜ ìˆœìˆ˜ íŒŒì´ì¬ìœ¼ë¡œ GPTë¥¼ í•™ìŠµí•˜ê³  ì¶”ë¡ í•˜ê¸°"
date: 2026-02-17 18:00:00 +0900
categories: ai
tags: [gpt, deep-learning, transformer, translation]
math: true
---

> ì´ ê¸€ì€ Andrej Karpathyì˜ [microgpt](https://karpathy.github.io/2026/02/12/microgpt/) í¬ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ ê²ƒì…ë‹ˆë‹¤.

ì´ ê¸€ì€ ì œ ìƒˆë¡œìš´ ì•„íŠ¸ í”„ë¡œì íŠ¸ [microgpt](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)ì— ëŒ€í•œ ê°„ë‹¨í•œ ê°€ì´ë“œì…ë‹ˆë‹¤. ì˜ì¡´ì„± ì—†ëŠ” ìˆœìˆ˜ Python 200ì¤„ì§œë¦¬ ë‹¨ì¼ íŒŒì¼ë¡œ GPTë¥¼ í•™ìŠµí•˜ê³  ì¶”ë¡ í•©ë‹ˆë‹¤. ì´ íŒŒì¼ì—ëŠ” í•„ìš”í•œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì  ë‚´ìš©ì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤: ë¬¸ì„œ ë°ì´í„°ì…‹, í† í¬ë‚˜ì´ì €, ìë™ ë¯¸ë¶„ ì—”ì§„, GPT-2 ìŠ¤íƒ€ì¼ì˜ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜, Adam ì˜µí‹°ë§ˆì´ì €, í•™ìŠµ ë£¨í”„, ì¶”ë¡  ë£¨í”„. ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ íš¨ìœ¨ì„±ì˜ ë¬¸ì œì…ë‹ˆë‹¤. ì´ê²ƒ ì´ìƒìœ¼ë¡œ ë‹¨ìˆœí™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì—¬ëŸ¬ í”„ë¡œì íŠ¸(micrograd, makemore, nanogpt ë“±)ì™€ LLMì„ í•µì‹¬ë§Œ ë‚¨ê¸°ë ¤ëŠ” 10ë…„ê°„ì˜ ì§‘ì°©ì˜ ê²°ì •ì²´ì´ë©°, ì €ëŠ” ì´ê²ƒì´ ì•„ë¦„ë‹µë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤ ğŸ¥¹. ì‹¬ì§€ì–´ 3ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ë‚˜ë‰˜ê¸°ê¹Œì§€ í•©ë‹ˆë‹¤:

![microgpt ì½”ë“œ](/assets/microgpt.jpg)

ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê³³:

- ì „ì²´ ì†ŒìŠ¤ ì½”ë“œê°€ ë‹´ê¸´ GitHub gist: [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)
- ì›¹ í˜ì´ì§€ì—ì„œë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤: [https://karpathy.ai/microgpt.html](https://karpathy.ai/microgpt.html)
- [Google Colab ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1vyN5zo6rqUp_dYNbT4Yrco66zuWCZKoN?usp=sharing)ìœ¼ë¡œë„ ì œê³µë©ë‹ˆë‹¤

ë‹¤ìŒì€ ê´€ì‹¬ ìˆëŠ” ë…ìë¥¼ ìœ„í•´ ì½”ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ë°ì´í„°ì…‹

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì—°ë£ŒëŠ” í…ìŠ¤íŠ¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì´ë©°, ì„ íƒì ìœ¼ë¡œ ì—¬ëŸ¬ ë¬¸ì„œë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ê¸‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê° ë¬¸ì„œëŠ” ì¸í„°ë„· ì›¹ í˜ì´ì§€ê°€ ë˜ê² ì§€ë§Œ, microgptì—ì„œëŠ” 32,000ê°œì˜ ì´ë¦„ìœ¼ë¡œ êµ¬ì„±ëœ ë” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# ì…ë ¥ ë°ì´í„°ì…‹ `docs`: list[str] í˜•íƒœì˜ ë¬¸ì„œë“¤ (ì˜ˆ: ì´ë¦„ ë°ì´í„°ì…‹)
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] ë¬¸ì„œë“¤
random.shuffle(docs)
print(f"num docs: {len(docs)}")
```

ë°ì´í„°ì…‹ì€ ì´ë ‡ê²Œ ìƒê²¼ìŠµë‹ˆë‹¤. ê° ì´ë¦„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œì…ë‹ˆë‹¤:

```
emma
olivia
ava
isabella
sophia
charlotte
mia
amelia
harper
... (~32,000ê°œì˜ ì´ë¦„ì´ ì´ì–´ì§‘ë‹ˆë‹¤)
```

ëª¨ë¸ì˜ ëª©í‘œëŠ” ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , ë™ì¼í•œ í†µê³„ì  íŒ¨í„´ì„ ê³µìœ í•˜ëŠ” ìƒˆë¡œìš´ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¯¸ë¦¬ ë³´ìë©´, ìŠ¤í¬ë¦½íŠ¸ì˜ ëì—ì„œ ìš°ë¦¬ ëª¨ë¸ì€ ìƒˆë¡œìš´, ê·¸ëŸ´ë“¯í•œ ì´ë¦„ë“¤ì„ ìƒì„±("í™˜ê°")í•©ë‹ˆë‹¤:

```
sample  1: kamon
sample  2: ann
sample  3: karai
sample  4: jaire
sample  5: vialan
sample  6: karia
sample  7: yeran
sample  8: anna
sample  9: areli
sample 10: kaina
sample 11: konna
sample 12: keylen
sample 13: liole
sample 14: alerin
sample 15: earan
sample 16: lenne
sample 17: kana
sample 18: lara
sample 19: alela
sample 20: anton
```

ë³„ê²ƒ ì•„ë‹Œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ChatGPT ê°™ì€ ëª¨ë¸ì˜ ê´€ì ì—ì„œ ë³´ë©´ ì—¬ëŸ¬ë¶„ê³¼ì˜ ëŒ€í™”ë„ ê·¸ì € ì¢€ íŠ¹ì´í•˜ê²Œ ìƒê¸´ "ë¬¸ì„œ"ì¼ ë¿ì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¡œ ë¬¸ì„œë¥¼ ì‹œì‘í•˜ë©´, ëª¨ë¸ì˜ ì‘ë‹µì€ ëª¨ë¸ ì…ì¥ì—ì„œ ê·¸ì € í†µê³„ì  ë¬¸ì„œ ì™„ì„±ì— ë¶ˆê³¼í•©ë‹ˆë‹¤.

## í† í¬ë‚˜ì´ì €

ë‚´ë¶€ì ìœ¼ë¡œ ì‹ ê²½ë§ì€ ë¬¸ìê°€ ì•„ë‹Œ ìˆ«ìë¡œ ì‘ë™í•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ í† í° id ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ê³  ë‹¤ì‹œ ë˜ëŒë¦¬ëŠ” ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. [tiktoken](https://github.com/openai/tiktoken)(GPT-4ì—ì„œ ì‚¬ìš©)ê³¼ ê°™ì€ í”„ë¡œë•ì…˜ í† í¬ë‚˜ì´ì €ëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¬¸ì ì²­í¬ ë‹¨ìœ„ë¡œ ì‘ë™í•˜ì§€ë§Œ, ê°€ì¥ ë‹¨ìˆœí•œ í† í¬ë‚˜ì´ì €ëŠ” ë°ì´í„°ì…‹ì˜ ê° ê³ ìœ  ë¬¸ìì— í•˜ë‚˜ì˜ ì •ìˆ˜ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤:

```python
# ë¬¸ìì—´ì„ ì´ì‚° ì‹¬ë³¼ë¡œ ë³€í™˜í•˜ê³  ë˜ëŒë¦¬ëŠ” í† í¬ë‚˜ì´ì €
uchars = sorted(set(''.join(docs))) # ë°ì´í„°ì…‹ì˜ ê³ ìœ  ë¬¸ìë“¤ì´ í† í° id 0..n-1ì´ ë¨
BOS = len(uchars) # íŠ¹ìˆ˜ ì‹œí€€ìŠ¤ ì‹œì‘(BOS) í† í°ì˜ í† í° id
vocab_size = len(uchars) + 1 # ê³ ìœ  í† í°ì˜ ì´ ìˆ˜, +1ì€ BOSìš©
print(f"vocab size: {vocab_size}")
```

ìœ„ ì½”ë“œì—ì„œ ë°ì´í„°ì…‹ ì „ì²´ì˜ ê³ ìœ  ë¬¸ìë“¤(ì†Œë¬¸ì a-z)ì„ ìˆ˜ì§‘í•˜ê³  ì •ë ¬í•œ ë’¤, ê° ë¬¸ìê°€ ì¸ë±ìŠ¤ë¡œ idë¥¼ ë°›ìŠµë‹ˆë‹¤. ì •ìˆ˜ ê°’ ìì²´ì—ëŠ” ì•„ë¬´ëŸ° ì˜ë¯¸ê°€ ì—†ë‹¤ëŠ” ì ì— ì£¼ëª©í•˜ì„¸ìš”. ê° í† í°ì€ ê·¸ì € ë³„ê°œì˜ ì´ì‚° ì‹¬ë³¼ì…ë‹ˆë‹¤. 0, 1, 2 ëŒ€ì‹  ì„œë¡œ ë‹¤ë¥¸ ì´ëª¨ì§€ì—¬ë„ ìƒê´€ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ `BOS`(Beginning of Sequence)ë¼ëŠ” íŠ¹ìˆ˜ í† í°ì„ í•˜ë‚˜ ë” ë§Œë“­ë‹ˆë‹¤. ì´ê²ƒì€ êµ¬ë¶„ì ì—­í• ì„ í•˜ì—¬ ëª¨ë¸ì—ê²Œ "ìƒˆ ë¬¸ì„œê°€ ì—¬ê¸°ì„œ ì‹œì‘/ëë‚©ë‹ˆë‹¤"ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ë‚˜ì¤‘ì— í•™ìŠµ ì‹œ ê° ë¬¸ì„œëŠ” ì–‘ìª½ì— `BOS`ë¡œ ê°ì‹¸ì§‘ë‹ˆë‹¤: `[BOS, e, m, m, a, BOS]`. ëª¨ë¸ì€ `BOS`ê°€ ìƒˆ ì´ë¦„ì„ ì‹œì‘í•˜ê³ , ë˜ ë‹¤ë¥¸ `BOS`ê°€ ëë‚¸ë‹¤ëŠ” ê²ƒì„ í•™ìŠµí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìµœì¢… ì–´íœ˜ëŠ” 27ê°œì…ë‹ˆë‹¤(ì†Œë¬¸ì a-z 26ê°œ + BOS í† í° 1ê°œ).

## ìë™ ë¯¸ë¶„ (Autograd)

ì‹ ê²½ë§ì„ í•™ìŠµí•˜ë ¤ë©´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤: ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ "ì´ ìˆ«ìë¥¼ ì¡°ê¸ˆ ì˜¬ë¦¬ë©´ ì†ì‹¤ì´ ì˜¬ë¼ê°€ë‚˜ ë‚´ë ¤ê°€ë‚˜, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜?"ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ê³„ì‚° ê·¸ë˜í”„ëŠ” ë§ì€ ì…ë ¥(ëª¨ë¸ íŒŒë¼ë¯¸í„°ì™€ ì…ë ¥ í† í°)ì„ ê°€ì§€ì§€ë§Œ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ì¶œë ¥ì¸ ì†ì‹¤(loss)ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤. ì—­ì „íŒŒëŠ” ê·¸ ë‹¨ì¼ ì¶œë ¥ì—ì„œ ì‹œì‘í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ê±°ê¾¸ë¡œ ë”°ë¼ê°€ë©°, ëª¨ë“  ì…ë ¥ì— ëŒ€í•œ ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¯¸ì ë¶„ì˜ ì—°ì‡„ ë²•ì¹™(chain rule)ì— ì˜ì¡´í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œëŠ” PyTorch ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `Value`ë¼ëŠ” ë‹¨ì¼ í´ë˜ìŠ¤ë¡œ ì²˜ìŒë¶€í„° êµ¬í˜„í•©ë‹ˆë‹¤:

```python
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads')

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # ìˆœì „íŒŒì—ì„œ ê³„ì‚°ëœ ì´ ë…¸ë“œì˜ ìŠ¤ì¹¼ë¼ ê°’
        self.grad = 0                   # ì´ ë…¸ë“œì— ëŒ€í•œ ì†ì‹¤ì˜ ë„í•¨ìˆ˜, ì—­ì „íŒŒì—ì„œ ê³„ì‚°
        self._children = children       # ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ì´ ë…¸ë“œì˜ ìì‹ë“¤
        self._local_grads = local_grads # ìì‹ì— ëŒ€í•œ ì´ ë…¸ë“œì˜ êµ­ì†Œ ë„í•¨ìˆ˜

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad
```

ì´ê²ƒì´ ìˆ˜í•™ì , ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œ ê°€ì¥ ì§‘ì•½ì ì¸ ë¶€ë¶„ì´ë©° ì´ì— ëŒ€í•œ 2.5ì‹œê°„ì§œë¦¬ ì˜ìƒì´ ìˆìŠµë‹ˆë‹¤: [micrograd ì˜ìƒ](https://www.youtube.com/watch?v=VMj-3S1tku0). ê°„ë‹¨íˆ ë§í•´, `Value`ëŠ” í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ìˆ«ì(`.data`)ë¥¼ ê°ì‹¸ê³  ê·¸ê²ƒì´ ì–´ë–»ê²Œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ ì¶”ì í•©ë‹ˆë‹¤. ê° ì—°ì‚°ì„ ì‘ì€ ë ˆê³  ë¸”ë¡ìœ¼ë¡œ ìƒê°í•˜ì„¸ìš”: ëª‡ ê°€ì§€ ì…ë ¥ì„ ë°›ì•„ ì¶œë ¥ì„ ìƒì„±í•˜ê³ (ìˆœì „íŒŒ), ê° ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ì˜ ë³€í™”ìœ¨(êµ­ì†Œ ê·¸ë˜ë””ì–¸íŠ¸)ì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ìë™ ë¯¸ë¶„ì´ ê° ë¸”ë¡ì—ì„œ í•„ìš”ë¡œ í•˜ëŠ” ì „ë¶€ì…ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ ì—°ì‡„ ë²•ì¹™, ì¦‰ ë¸”ë¡ë“¤ì„ ì—°ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

`Value` ê°ì²´ë¡œ ìˆ˜í•™ ì—°ì‚°(ë”í•˜ê¸°, ê³±í•˜ê¸° ë“±)ì„ í•  ë•Œë§ˆë‹¤, ê²°ê³¼ëŠ” ìì‹ ì˜ ì…ë ¥(`_children`)ê³¼ í•´ë‹¹ ì—°ì‚°ì˜ êµ­ì†Œ ë„í•¨ìˆ˜(`_local_grads`)ë¥¼ ê¸°ì–µí•˜ëŠ” ìƒˆë¡œìš´ `Value`ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `__mul__`ì€ $$\frac{\partial(a \cdot b)}{\partial a} = b$$ ì™€ $$\frac{\partial(a \cdot b)}{\partial b} = a$$ ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì „ì²´ ë ˆê³  ë¸”ë¡ ëª©ë¡:

| ì—°ì‚° | ìˆœì „íŒŒ | êµ­ì†Œ ê·¸ë˜ë””ì–¸íŠ¸ |
|------|--------|----------------|
| `a + b` | $$a + b$$ | $$\frac{\partial}{\partial a} = 1, \quad \frac{\partial}{\partial b} = 1$$ |
| `a * b` | $$a \cdot b$$ | $$\frac{\partial}{\partial a} = b, \quad \frac{\partial}{\partial b} = a$$ |
| `a ** n` | $$a^n$$ | $$\frac{\partial}{\partial a} = n \cdot a^{n-1}$$ |
| `log(a)` | $$\ln(a)$$ | $$\frac{\partial}{\partial a} = \frac{1}{a}$$ |
| `exp(a)` | $$e^a$$ | $$\frac{\partial}{\partial a} = e^a$$ |
| `relu(a)` | $$\max(0, a)$$ | $$\frac{\partial}{\partial a} = \mathbf{1}_{a > 0}$$ |

`backward()` ë©”ì„œë“œëŠ” ì´ ê·¸ë˜í”„ë¥¼ ì—­ ìœ„ìƒ ì •ë ¬ ìˆœì„œë¡œ ìˆœíšŒí•©ë‹ˆë‹¤(ì†ì‹¤ì—ì„œ ì‹œì‘í•˜ì—¬ íŒŒë¼ë¯¸í„°ì—ì„œ ëë‚¨). ê° ë‹¨ê³„ì—ì„œ ì—°ì‡„ ë²•ì¹™ì„ ì ìš©í•©ë‹ˆë‹¤. ì†ì‹¤ì´ $$L$$ì´ê³  ë…¸ë“œ $$v$$ê°€ êµ­ì†Œ ê·¸ë˜ë””ì–¸íŠ¸ $$\frac{\partial v}{\partial c}$$ë¥¼ ê°€ì§„ ìì‹ $$c$$ë¥¼ ê°–ëŠ”ë‹¤ë©´:

$$\frac{\partial L}{\partial c} \mathrel{+}= \frac{\partial v}{\partial c} \cdot \frac{\partial L}{\partial v}$$

ë¯¸ì ë¶„ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ ë¬´ì„œì›Œ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ì‚¬ì‹¤ ì§ê´€ì ìœ¼ë¡œ ë‘ ìˆ˜ë¥¼ ê³±í•˜ëŠ” ê²ƒì¼ ë¿ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ë³´ë©´ ë©ë‹ˆë‹¤: "ìë™ì°¨ê°€ ìì „ê±°ë³´ë‹¤ 2ë°° ë¹ ë¥´ê³ , ìì „ê±°ê°€ ê±·ëŠ” ì‚¬ëŒë³´ë‹¤ 4ë°° ë¹ ë¥´ë©´, ìë™ì°¨ëŠ” ì‚¬ëŒë³´ë‹¤ 2 x 4 = 8ë°° ë¹ ë¥´ë‹¤." ì—°ì‡„ ë²•ì¹™ë„ ê°™ì€ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤: ê²½ë¡œë¥¼ ë”°ë¼ ë³€í™”ìœ¨ì„ ê³±í•©ë‹ˆë‹¤.

ì†ì‹¤ ë…¸ë“œì—ì„œ `self.grad = 1`ì„ ì„¤ì •í•˜ì—¬ ì‹œì‘í•©ë‹ˆë‹¤. $$\frac{\partial L}{\partial L} = 1$$ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤: ì†ì‹¤ì˜ ìê¸° ìì‹ ì— ëŒ€í•œ ë³€í™”ìœ¨ì€ ë‹¹ì—°íˆ 1ì…ë‹ˆë‹¤. ê±°ê¸°ì„œë¶€í„° ì—°ì‡„ ë²•ì¹™ì´ íŒŒë¼ë¯¸í„°ê¹Œì§€ì˜ ëª¨ë“  ê²½ë¡œë¥¼ ë”°ë¼ êµ­ì†Œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³±í•©ë‹ˆë‹¤.

`+=`(ëŒ€ì…ì´ ì•„ë‹Œ ëˆ„ì )ì— ì£¼ëª©í•˜ì„¸ìš”. ê°’ì´ ê·¸ë˜í”„ì˜ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš©ë  ë•Œ(ì¦‰ ê·¸ë˜í”„ê°€ ë¶„ê¸°í•  ë•Œ), ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ê° ë¶„ê¸°ë¥¼ ë”°ë¼ ë…ë¦½ì ìœ¼ë¡œ íë¥´ë©° í•©ì‚°ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë‹¤ë³€ìˆ˜ ì—°ì‡„ ë²•ì¹™ì˜ ê²°ê³¼ì…ë‹ˆë‹¤.

`backward()`ê°€ ì™„ë£Œë˜ë©´, ê·¸ë˜í”„ì˜ ëª¨ë“  `Value`ëŠ” $$\frac{\partial L}{\partial v}$$ë¥¼ ë‹´ê³  ìˆëŠ” `.grad`ë¥¼ ê°–ê²Œ ë˜ë©°, ì´ëŠ” í•´ë‹¹ ê°’ì„ ì‚´ì§ ë³€ê²½í–ˆì„ ë•Œ ìµœì¢… ì†ì‹¤ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.

êµ¬ì²´ì ì¸ ì˜ˆì‹œì…ë‹ˆë‹¤. `a`ê°€ ë‘ ë²ˆ ì‚¬ìš©ë˜ë¯€ë¡œ(ê·¸ë˜í”„ê°€ ë¶„ê¸°) ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì–‘ìª½ ê²½ë¡œì˜ í•©ì…ë‹ˆë‹¤:

```python
a = Value(2.0)
b = Value(3.0)
c = a * b       # c = 6.0
L = c + a       # L = 8.0
L.backward()
print(a.grad)   # 4.0 (dL/da = b + 1 = 3 + 1, ì–‘ìª½ ê²½ë¡œë¥¼ í†µí•´)
print(b.grad)   # 2.0 (dL/db = a = 2)
```

ì´ê²ƒì€ PyTorchì˜ `.backward()`ê°€ ì£¼ëŠ” ê²ƒê³¼ ì •í™•íˆ ê°™ìŠµë‹ˆë‹¤:

```python
import torch
a = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(3.0, requires_grad=True)
c = a * b
L = c + a
L.backward()
print(a.grad)   # tensor(4.)
print(b.grad)   # tensor(2.)
```

PyTorchì˜ `loss.backward()`ê°€ ì‹¤í–‰í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ì´ë©°, í…ì„œ(ìŠ¤ì¹¼ë¼ ë°°ì—´) ëŒ€ì‹  ìŠ¤ì¹¼ë¼ì—ì„œ ì‘ë™í•  ë¿ì…ë‹ˆë‹¤ - ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œ ë™ì¼í•˜ê³ , í›¨ì”¬ ì‘ê³  ë‹¨ìˆœí•˜ì§€ë§Œ, ë¬¼ë¡  í›¨ì”¬ ëœ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

## íŒŒë¼ë¯¸í„°

íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ì˜ ì§€ì‹ì…ë‹ˆë‹¤. ë¬´ì‘ìœ„ë¡œ ì‹œì‘í•˜ì—¬ í•™ìŠµ ì¤‘ ë°˜ë³µì ìœ¼ë¡œ ìµœì í™”ë˜ëŠ” ë¶€ë™ì†Œìˆ˜ì  ìˆ«ìë“¤ì˜ í° ì»¬ë ‰ì…˜ì…ë‹ˆë‹¤(ìë™ ë¯¸ë¶„ì„ ìœ„í•´ `Value`ë¡œ ê°ì‹¸ì ¸ ìˆìŒ). ê° íŒŒë¼ë¯¸í„°ì˜ ì •í™•í•œ ì—­í• ì€ ì•„ë˜ì—ì„œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•˜ë©´ ë” ì´í•´ê°€ ë˜ê² ì§€ë§Œ, ì§€ê¸ˆì€ ì´ˆê¸°í™”ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:

```python
n_embd = 16     # ì„ë² ë”© ì°¨ì›
n_head = 4      # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
n_layer = 1     # ë ˆì´ì–´ ìˆ˜
block_size = 16 # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
head_dim = n_embd // n_head # ê° í—¤ë“œì˜ ì°¨ì›
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)
params = [p for mat in state_dict.values() for row in mat for p in row]
print(f"num params: {len(params)}")
```

ê° íŒŒë¼ë¯¸í„°ëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ë½‘ì€ ì‘ì€ ë‚œìˆ˜ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. `state_dict`ëŠ” ì´ë“¤ì„ ëª…ëª…ëœ í–‰ë ¬ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤(PyTorch ìš©ì–´ë¥¼ ì°¨ìš©): ì„ë² ë”© í…Œì´ë¸”, ì–´í…ì…˜ ê°€ì¤‘ì¹˜, MLP ê°€ì¤‘ì¹˜, ìµœì¢… ì¶œë ¥ í”„ë¡œì ì…˜. ë‚˜ì¤‘ì— ì˜µí‹°ë§ˆì´ì €ê°€ ìˆœíšŒí•  ìˆ˜ ìˆë„ë¡ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ `params`ë¡œ í‰íƒ„í™”í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‘ì€ ëª¨ë¸ì—ì„œ ì´ëŠ” 4,192ê°œì˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. GPT-2ëŠ” 16ì–µ ê°œ, í˜„ëŒ€ LLMì€ ìˆ˜ì²œì–µ ê°œë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## ì•„í‚¤í…ì²˜

ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ìƒíƒœ ì—†ëŠ”(stateless) í•¨ìˆ˜ì…ë‹ˆë‹¤: í† í°, ìœ„ì¹˜, íŒŒë¼ë¯¸í„°, ì´ì „ ìœ„ì¹˜ì˜ ìºì‹œëœ í‚¤/ê°’ì„ ë°›ì•„ì„œ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¬ í† í°ì— ëŒ€í•œ ë¡œì§“(ì ìˆ˜)ì„ ë°˜í™˜í•©ë‹ˆë‹¤. GPT-2ë¥¼ ë”°ë¥´ë˜ ì•½ê°„ì˜ ë‹¨ìˆœí™”ë¥¼ í•©ë‹ˆë‹¤: LayerNorm ëŒ€ì‹  RMSNorm, ë°”ì´ì–´ìŠ¤ ì—†ìŒ, GeLU ëŒ€ì‹  ReLU. ë¨¼ì € ì„¸ ê°€ì§€ ì‘ì€ í—¬í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤:

```python
def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]
```

`linear`ëŠ” í–‰ë ¬-ë²¡í„° ê³±ì…ë‹ˆë‹¤. ë²¡í„° `x`ì™€ ê°€ì¤‘ì¹˜ í–‰ë ¬ `w`ë¥¼ ë°›ì•„ `w`ì˜ ê° í–‰ì— ëŒ€í•´ ë‚´ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ê²ƒì´ ì‹ ê²½ë§ì˜ ê¸°ë³¸ ë¹Œë”© ë¸”ë¡ì…ë‹ˆë‹¤: í•™ìŠµëœ ì„ í˜• ë³€í™˜.

```python
def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]
```

`softmax`ëŠ” $$-\infty$$ì—ì„œ $$+\infty$$ ë²”ìœ„ì˜ ì›ì‹œ ì ìˆ˜(ë¡œì§“) ë²¡í„°ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤: ëª¨ë“  ê°’ì´ $$[0, 1]$$ ë²”ìœ„ê°€ ë˜ê³  í•©ì´ 1ì´ ë©ë‹ˆë‹¤. ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¨¼ì € ìµœëŒ“ê°’ì„ ë¹¼ì¤ë‹ˆë‹¤(ìˆ˜í•™ì ìœ¼ë¡œ ê²°ê³¼ëŠ” ë³€í•˜ì§€ ì•Šì§€ë§Œ `exp`ì—ì„œ ì˜¤ë²„í”Œë¡œìš°ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤).

```python
def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]
```

`rmsnorm`(Root Mean Square Normalization)ì€ ë²¡í„°ë¥¼ ë‹¨ìœ„ RMSë¥¼ ê°–ë„ë¡ ì¬ì¡°ì •í•©ë‹ˆë‹¤. ì´ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´ì„œ í™œì„±í™”ê°€ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™”í•©ë‹ˆë‹¤. ì›ë˜ GPT-2ì—ì„œ ì‚¬ìš©ëœ LayerNormì˜ ë” ë‹¨ìˆœí•œ ë³€í˜•ì…ë‹ˆë‹¤.

ì´ì œ ëª¨ë¸ ìì²´ì…ë‹ˆë‹¤:

```python
def gpt(token_id, pos_id, keys, values):
    tok_emb = state_dict['wte'][token_id] # í† í° ì„ë² ë”©
    pos_emb = state_dict['wpe'][pos_id] # ìœ„ì¹˜ ì„ë² ë”©
    x = [t + p for t, p in zip(tok_emb, pos_emb)] # í† í°ê³¼ ìœ„ì¹˜ ì„ë² ë”© ê²°í•©
    x = rmsnorm(x)

    for li in range(n_layer):
        # 1) ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ë¸”ë¡
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])
        k = linear(x, state_dict[f'layer{li}.attn_wk'])
        v = linear(x, state_dict[f'layer{li}.attn_wv'])
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]
        # 2) MLP ë¸”ë¡
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])
        x = [a + b for a, b in zip(x, x_residual)]

    logits = linear(x, state_dict['lm_head'])
    return logits
```

ì´ í•¨ìˆ˜ëŠ” íŠ¹ì • ìœ„ì¹˜(`pos_id`)ì— ìˆëŠ” í•˜ë‚˜ì˜ í† í°(`token_id`)ì„ ì²˜ë¦¬í•˜ë©°, KV ìºì‹œë¼ê³  ì•Œë ¤ì§„ `keys`ì™€ `values`ì˜ í™œì„±í™”ë¡œ ìš”ì•½ëœ ì´ì „ ë°˜ë³µì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

**ì„ë² ë”©.** ì‹ ê²½ë§ì€ 5 ê°™ì€ ì›ì‹œ í† í° idë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°(ìˆ«ì ë¦¬ìŠ¤íŠ¸)ë§Œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ê°€ëŠ¥í•œ ê° í† í°ì— í•™ìŠµëœ ë²¡í„°ë¥¼ ì—°ê²°í•˜ê³  ê·¸ê²ƒì„ ì‹ ê²½ ì‹œê·¸ë‹ˆì²˜ë¡œ ì…ë ¥í•©ë‹ˆë‹¤. í† í° idì™€ ìœ„ì¹˜ idê°€ ê°ê°ì˜ ì„ë² ë”© í…Œì´ë¸”(`wte`ì™€ `wpe`)ì—ì„œ í–‰ì„ ì°¾ìŠµë‹ˆë‹¤. ì´ ë‘ ë²¡í„°ë¥¼ ë”í•˜ë©´ í† í°ì´ *ë¬´ì—‡*ì¸ì§€ì™€ ì‹œí€€ìŠ¤ì—ì„œ *ì–´ë””*ì— ìˆëŠ”ì§€ë¥¼ ëª¨ë‘ ì¸ì½”ë”©í•˜ëŠ” í‘œí˜„ì„ ì–»ìŠµë‹ˆë‹¤.

**ì–´í…ì…˜ ë¸”ë¡.** í˜„ì¬ í† í°ì€ ì„¸ ê°€ì§€ ë²¡í„°ë¡œ í”„ë¡œì ì…˜ë©ë‹ˆë‹¤: ì¿¼ë¦¬(Q), í‚¤(K), ê°’(V). ì§ê´€ì ìœ¼ë¡œ, ì¿¼ë¦¬ëŠ” "ë‚˜ëŠ” ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?", í‚¤ëŠ” "ë‚˜ëŠ” ë¬´ì—‡ì„ ë‹´ê³  ìˆëŠ”ê°€?", ê°’ì€ "ì„ íƒë˜ë©´ ë¬´ì—‡ì„ ì œê³µí•˜ëŠ”ê°€?"ë¼ê³  ë§í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì´ë¦„ "emma"ì—ì„œ ëª¨ë¸ì´ ë‘ ë²ˆì§¸ "m"ì— ìˆìœ¼ë©´ì„œ ë‹¤ìŒì— ë¬´ì—‡ì´ ì˜¬ì§€ ì˜ˆì¸¡í•˜ë ¤ í•  ë•Œ, "ìµœê·¼ì— ì–´ë–¤ ëª¨ìŒì´ ë‚˜ì™”ì§€?" ê°™ì€ ì¿¼ë¦¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ì˜ "e"ëŠ” ì´ ì¿¼ë¦¬ì™€ ì˜ ë§ëŠ” í‚¤ë¥¼ ê°€ì§€ë¯€ë¡œ ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë°›ê³ , ê·¸ ê°’(ëª¨ìŒì´ë¼ëŠ” ì •ë³´)ì´ í˜„ì¬ ìœ„ì¹˜ë¡œ íë¦…ë‹ˆë‹¤. ì–´í…ì…˜ ë¸”ë¡ì€ ìœ„ì¹˜ `t`ì˜ í† í°ì´ ê³¼ê±° `0..t-1`ì˜ í† í°ì„ "ë³¼" ìˆ˜ ìˆëŠ” ì •í™•í•˜ê³  ìœ ì¼í•œ ì¥ì†Œë¼ëŠ” ì ì„ ê°•ì¡°í•  ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ì–´í…ì…˜ì€ í† í° í†µì‹  ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

**MLP ë¸”ë¡.** MLPëŠ” ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (multilayer perceptron)ì˜ ì•½ìì´ë©°, 2ì¸µ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤: ì„ë² ë”© ì°¨ì›ì˜ 4ë°°ë¡œ í™•ì¥í•˜ê³ , ReLUë¥¼ ì ìš©í•˜ê³ , ë‹¤ì‹œ ì¶•ì†Œí•©ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë¸ì´ ê° ìœ„ì¹˜ì—ì„œ ëŒ€ë¶€ë¶„ì˜ "ì‚¬ê³ "ë¥¼ í•˜ëŠ” ê³³ì…ë‹ˆë‹¤. ì–´í…ì…˜ê³¼ ë‹¬ë¦¬ ì´ ê³„ì‚°ì€ ì‹œê°„ `t`ì— ì™„ì „íˆ ë¡œì»¬ì…ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í†µì‹ (ì–´í…ì…˜)ê³¼ ê³„ì‚°(MLP)ì„ ë²ˆê°ˆì•„ ë°°ì¹˜í•©ë‹ˆë‹¤.

**ì”ì°¨ ì—°ê²°.** ì–´í…ì…˜ê³¼ MLP ë¸”ë¡ ëª¨ë‘ ì¶œë ¥ì„ ì…ë ¥ì— ë‹¤ì‹œ ë”í•©ë‹ˆë‹¤(`x = [a + b for ...]`). ì´ë¥¼ í†µí•´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§ì ‘ í†µê³¼í•˜ì—¬ ë” ê¹Šì€ ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

**ì¶œë ¥.** ìµœì¢… ì€ë‹‰ ìƒíƒœê°€ `lm_head`ì— ì˜í•´ ì–´íœ˜ í¬ê¸°ë¡œ í”„ë¡œì ì…˜ë˜ì–´ ì–´íœ˜ì˜ ê° í† í°ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë¡œì§“ì„ ìƒì„±í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²½ìš° ë‹¨ 27ê°œì˜ ìˆ«ìì…ë‹ˆë‹¤. ë¡œì§“ì´ ë†’ì„ìˆ˜ë¡ = ëª¨ë¸ì´ í•´ë‹¹ í† í°ì´ ë‹¤ìŒì— ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.

## í•™ìŠµ ë£¨í”„

ì´ì œ ëª¨ë“  ê²ƒì„ ì—°ê²°í•©ë‹ˆë‹¤. í•™ìŠµ ë£¨í”„ëŠ” ë°˜ë³µì ìœ¼ë¡œ: (1) ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³ , (2) í† í°ì— ëŒ€í•´ ëª¨ë¸ì„ ìˆœì „íŒŒí•˜ê³ , (3) ì†ì‹¤ì„ ê³„ì‚°í•˜ê³ , (4) ì—­ì „íŒŒë¡œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì–»ê³ , (5) íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

```python
# Adam, ì¶•ë³µë°›ì€ ì˜µí‹°ë§ˆì´ì €ì™€ ê·¸ ë²„í¼
learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8
m = [0.0] * len(params) # 1ì°¨ ëª¨ë©˜íŠ¸ ë²„í¼
v = [0.0] * len(params) # 2ì°¨ ëª¨ë©˜íŠ¸ ë²„í¼

# ìˆœì°¨ì ìœ¼ë¡œ ë°˜ë³µ
num_steps = 1000 # í•™ìŠµ ìŠ¤í… ìˆ˜
for step in range(num_steps):

    # ë‹¨ì¼ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ í† í¬ë‚˜ì´ì¦ˆí•˜ê³ , ì–‘ìª½ì„ BOS íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ ê°ìŒˆ
    doc = docs[step % len(docs)]
    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
    n = min(block_size, len(tokens) - 1)

    # í† í° ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ì— ìˆœì „íŒŒí•˜ì—¬ ì†ì‹¤ê¹Œì§€ì˜ ê³„ì‚° ê·¸ë˜í”„ë¥¼ êµ¬ì¶•
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    losses = []
    for pos_id in range(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax(logits)
        loss_t = -probs[target_id].log()
        losses.append(loss_t)
    loss = (1 / n) * sum(losses) # ë¬¸ì„œ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ìµœì¢… í‰ê·  ì†ì‹¤

    # ì†ì‹¤ì„ ì—­ì „íŒŒí•˜ì—¬ ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
    loss.backward()

    # Adam ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸: í•´ë‹¹ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
    lr_t = learning_rate * (1 - step / num_steps) # ì„ í˜• í•™ìŠµë¥  ê°ì†Œ
    for i, p in enumerate(params):
        m[i] = beta1 * m[i] + (1 - beta1) * p.grad
        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2
        m_hat = m[i] / (1 - beta1 ** (step + 1))
        v_hat = v[i] / (1 - beta2 ** (step + 1))
        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)
        p.grad = 0

    print(f"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}")
```

ê° ë¶€ë¶„ì„ ì‚´í´ë´…ì‹œë‹¤:

**í† í¬ë‚˜ì´ì œì´ì…˜.** ê° í•™ìŠµ ìŠ¤í…ì€ í•˜ë‚˜ì˜ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³  ì–‘ìª½ì„ `BOS`ë¡œ ê°ìŒ‰ë‹ˆë‹¤: ì´ë¦„ "emma"ëŠ” `[BOS, e, m, m, a, BOS]`ê°€ ë©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì„ë¬´ëŠ” ì´ì „ í† í°ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ìˆœì „íŒŒì™€ ì†ì‹¤.** í† í°ì„ í•œ ë²ˆì— í•˜ë‚˜ì”© ëª¨ë¸ì— ì…ë ¥í•˜ë©´ì„œ KV ìºì‹œë¥¼ ìŒ“ì•„ê°‘ë‹ˆë‹¤. ê° ìœ„ì¹˜ì—ì„œ ëª¨ë¸ì€ 27ê°œì˜ ë¡œì§“ì„ ì¶œë ¥í•˜ê³ , softmaxë¡œ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° ìœ„ì¹˜ì˜ ì†ì‹¤ì€ ì •ë‹µ ë‹¤ìŒ í† í°ì˜ ìŒì˜ ë¡œê·¸ í™•ë¥ ì…ë‹ˆë‹¤: $$-\log p(\text{target})$$. ì´ê²ƒì„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì´ë¼ê³  í•©ë‹ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ ì†ì‹¤ì€ ì˜¤ì˜ˆì¸¡ì˜ ì •ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤: ëª¨ë¸ì´ ì‹¤ì œë¡œ ë‹¤ìŒì— ì˜¤ëŠ” ê²ƒì— ì–¼ë§ˆë‚˜ ë†€ëëŠ”ì§€. ëª¨ë¸ì´ ì •ë‹µ í† í°ì— í™•ë¥  1.0ì„ ë¶€ì—¬í•˜ë©´ ì „í˜€ ë†€ë¼ì§€ ì•Šê³  ì†ì‹¤ì€ 0ì…ë‹ˆë‹¤. í™•ë¥ ì´ 0ì— ê°€ê¹Œìš°ë©´ ëª¨ë¸ì€ ë§¤ìš° ë†€ë¼ê³  ì†ì‹¤ì€ $$+\infty$$ë¡œ ê°‘ë‹ˆë‹¤.

**ì—­ì „íŒŒ.** `loss.backward()` í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ì „ì²´ ê³„ì‚° ê·¸ë˜í”„ë¥¼ í†µí•´ ì—­ì „íŒŒê°€ ì‹¤í–‰ë©ë‹ˆë‹¤. ì´í›„ ê° íŒŒë¼ë¯¸í„°ì˜ `.grad`ê°€ ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•´ ì–´ë–»ê²Œ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.

**Adam ì˜µí‹°ë§ˆì´ì €.** ë‹¨ìˆœíˆ `p.data -= lr * p.grad`(ê²½ì‚¬ í•˜ê°•ë²•)ë¥¼ í•  ìˆ˜ë„ ìˆì§€ë§Œ, Adamì€ ë” ë˜‘ë˜‘í•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë‹¹ ë‘ ê°€ì§€ ì´ë™ í‰ê· ì„ ìœ ì§€í•©ë‹ˆë‹¤: `m`ì€ ìµœê·¼ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í‰ê· ì„ ì¶”ì í•˜ê³ (ëª¨ë©˜í…€, êµ´ëŸ¬ê°€ëŠ” ê³µì²˜ëŸ¼), `v`ëŠ” ìµœê·¼ ê·¸ë˜ë””ì–¸íŠ¸ ì œê³±ì˜ í‰ê· ì„ ì¶”ì í•©ë‹ˆë‹¤(íŒŒë¼ë¯¸í„°ë³„ í•™ìŠµë¥  ì¡°ì •). `m_hat`ê³¼ `v_hat`ì€ `m`ê³¼ `v`ê°€ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ì›Œë°ì—…ì´ í•„ìš”í•œ ê²ƒì„ ë³´ì •í•˜ëŠ” ë°”ì´ì–´ìŠ¤ ë³´ì •ì…ë‹ˆë‹¤. í•™ìŠµë¥ ì€ í•™ìŠµ ì „ì²´ì— ê±¸ì³ ì„ í˜•ìœ¼ë¡œ ê°ì†Œí•©ë‹ˆë‹¤.

1,000 ìŠ¤í… ë™ì•ˆ ì†ì‹¤ì€ ì•½ 3.3(27ê°œ í† í° ì¤‘ ë¬´ì‘ìœ„ ì¶”ì¸¡: $$-\log(1/27) \approx 3.3$$)ì—ì„œ ì•½ 2.37ë¡œ ê°ì†Œí•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì¢‹ê³  ê°€ëŠ¥í•œ ìµœì €ê°’ì€ 0(ì™„ë²½í•œ ì˜ˆì¸¡)ì´ë¯€ë¡œ ê°œì„  ì—¬ì§€ê°€ ìˆì§€ë§Œ, ëª¨ë¸ì´ ì´ë¦„ì˜ í†µê³„ì  íŒ¨í„´ì„ ë¶„ëª…íˆ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ì¶”ë¡ 

í•™ìŠµì´ ëë‚˜ë©´ ëª¨ë¸ì—ì„œ ìƒˆë¡œìš´ ì´ë¦„ì„ ìƒ˜í”Œë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •ë˜ê³  ìˆœì „íŒŒë§Œ ë£¨í”„ì—ì„œ ì‹¤í–‰í•˜ë©°, ìƒì„±ëœ ê° í† í°ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ ë„£ìŠµë‹ˆë‹¤:

```python
temperature = 0.5 # (0, 1] ë²”ìœ„, ìƒì„± í…ìŠ¤íŠ¸ì˜ "ì°½ì˜ì„±" ì¡°ì ˆ, ë‚®ì€ ê°’ì—ì„œ ë†’ì€ ê°’ìœ¼ë¡œ
print("\n--- inference (new, hallucinated names) ---")
for sample_idx in range(20):
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    token_id = BOS
    sample = []
    for pos_id in range(block_size):
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax([l / temperature for l in logits])
        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]
        if token_id == BOS:
            break
        sample.append(uchars[token_id])
    print(f"sample {sample_idx+1:2d}: {''.join(sample)}")
```

ê° ìƒ˜í”Œì„ `BOS` í† í°ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ëª¨ë¸ì—ê²Œ "ìƒˆ ì´ë¦„ì„ ì‹œì‘í•˜ë¼"ê³  ì•Œë¦½ë‹ˆë‹¤. ëª¨ë¸ì´ 27ê°œì˜ ë¡œì§“ì„ ìƒì„±í•˜ë©´ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³ , ê·¸ í™•ë¥ ì— ë”°ë¼ í•˜ë‚˜ì˜ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ê·¸ í† í°ì´ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ ë“¤ì–´ê°€ê³ , ëª¨ë¸ì´ ë‹¤ì‹œ `BOS`ë¥¼ ìƒì„±í•˜ê±°ë‚˜("ëë‚¬ë‹¤") ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.

`temperature` íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì‘ìœ„ì„±ì„ ì œì–´í•©ë‹ˆë‹¤. softmax ì „ì— ë¡œì§“ì„ ì˜¨ë„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ì˜¨ë„ 1.0ì€ ëª¨ë¸ì˜ í•™ìŠµëœ ë¶„í¬ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ë‚®ì€ ì˜¨ë„(ì—¬ê¸°ì„œëŠ” 0.5)ëŠ” ë¶„í¬ë¥¼ ë‚ ì¹´ë¡­ê²Œ ë§Œë“¤ì–´ ëª¨ë¸ì´ ìƒìœ„ ì„ íƒì§€ë¥¼ ì„ í˜¸í•˜ê²Œ í•©ë‹ˆë‹¤. 0ì— ê°€ê¹Œìš´ ì˜¨ë„ëŠ” í•­ìƒ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤(ê·¸ë¦¬ë”” ë””ì½”ë”©). ë†’ì€ ì˜¨ë„ëŠ” ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ í•˜ì—¬ ë” ë‹¤ì–‘í•˜ì§€ë§Œ ëœ ì¼ê´€ëœ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

## ì‹¤í–‰í•˜ê¸°

Pythonë§Œ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤(pip install ì—†ìŒ, ì˜ì¡´ì„± ì—†ìŒ):

```bash
python train.py
```

ìŠ¤í¬ë¦½íŠ¸ëŠ” ë§¥ë¶ì—ì„œ ì•½ 1ë¶„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤. ê° ìŠ¤í…ë§ˆë‹¤ ì†ì‹¤ì´ ì¶œë ¥ë©ë‹ˆë‹¤:

```
train.py
num docs: 32033
vocab size: 27
num params: 4192
step    1 / 1000 | loss 3.3660
step    2 / 1000 | loss 3.4243
step    3 / 1000 | loss 3.1778
...
```

~3.3(ë¬´ì‘ìœ„)ì—ì„œ ~2.37ë¡œ ë‚´ë ¤ê°€ëŠ” ê²ƒì„ ì§€ì¼œë³´ì„¸ìš”. í•™ìŠµì´ ëë‚˜ë©´ í•™ìŠµ í† í° ì‹œí€€ìŠ¤ì˜ í†µê³„ì  íŒ¨í„´ì— ëŒ€í•œ ì§€ì‹ì´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì¦ë¥˜ë©ë‹ˆë‹¤.

## ë‹¨ê³„ë³„ ì§„í–‰

ì½”ë“œê°€ ì–‘íŒŒì˜ ì¸µì²˜ëŸ¼ í•˜ë‚˜ì”© ìŒ“ì´ëŠ” ê²ƒì„ ë³´ë ¤ë©´, ê¶Œì¥í•˜ëŠ” ì§„í–‰ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

| íŒŒì¼ | ì¶”ê°€í•˜ëŠ” ê²ƒ |
|------|-------------|
| `train0.py` | ë°”ì´ê·¸ë¨ ì¹´ìš´íŠ¸ í…Œì´ë¸” â€” ì‹ ê²½ë§ ì—†ìŒ, ê·¸ë˜ë””ì–¸íŠ¸ ì—†ìŒ |
| `train1.py` | MLP + ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸(ìˆ˜ì¹˜ì  & í•´ì„ì ) + SGD |
| `train2.py` | ìë™ ë¯¸ë¶„(Value í´ë˜ìŠ¤) â€” ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸ ëŒ€ì²´ |
| `train3.py` | ìœ„ì¹˜ ì„ë² ë”© + ë‹¨ì¼ í—¤ë“œ ì–´í…ì…˜ + rmsnorm + ì”ì°¨ |
| `train4.py` | ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ + ë ˆì´ì–´ ë£¨í”„ â€” ì „ì²´ GPT ì•„í‚¤í…ì²˜ |
| `train5.py` | Adam ì˜µí‹°ë§ˆì´ì € â€” ì´ê²ƒì´ `train.py` |

[build_microgpt.py](https://gist.github.com/karpathy/561ac2de12a47cc06a23691e1be9543a)ë¼ëŠ” Gistë¥¼ ë§Œë“¤ì—ˆëŠ”ë°, Revisionsì—ì„œ ì´ ëª¨ë“  ë²„ì „ê³¼ ê° ë‹¨ê³„ ê°„ì˜ diffë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‹¤ì œ í”„ë¡œë•ì…˜ê³¼ì˜ ì°¨ì´

microgptëŠ” GPTë¥¼ í•™ìŠµí•˜ê³  ì‹¤í–‰í•˜ëŠ” ì™„ì „í•œ ì•Œê³ ë¦¬ì¦˜ì  ë³¸ì§ˆì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ê²ƒê³¼ ChatGPT ê°™ì€ í”„ë¡œë•ì…˜ LLM ì‚¬ì´ì—ëŠ” ë³€ê²½ë˜ëŠ” ê²ƒë“¤ì˜ ê¸´ ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ì „ì²´ êµ¬ì¡°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì€ ì—†ì§€ë§Œ, ëŒ€ê·œëª¨ì—ì„œ ì‹¤ì œë¡œ ì‘ë™í•˜ê²Œ ë§Œë“œëŠ” ê²ƒë“¤ì…ë‹ˆë‹¤:

**ë°ì´í„°.** 32Kê°œì˜ ì§§ì€ ì´ë¦„ ëŒ€ì‹ , í”„ë¡œë•ì…˜ ëª¨ë¸ì€ ìˆ˜ì¡° í† í°ì˜ ì¸í„°ë„· í…ìŠ¤íŠ¸ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ë°ì´í„°ëŠ” ì¤‘ë³µ ì œê±°ë˜ê³ , í’ˆì§ˆ í•„í„°ë§ë˜ë©°, ë„ë©”ì¸ ê°„ ì‹ ì¤‘í•˜ê²Œ í˜¼í•©ë©ë‹ˆë‹¤.

**í† í¬ë‚˜ì´ì €.** ë‹¨ì¼ ë¬¸ì ëŒ€ì‹ , í”„ë¡œë•ì…˜ ëª¨ë¸ì€ BPE(Byte Pair Encoding) ê°™ì€ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ~100K í† í°ì˜ ì–´íœ˜ë¥¼ ì œê³µí•˜ë©° ìœ„ì¹˜ë‹¹ ë” ë§ì€ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆì–´ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

**ìë™ ë¯¸ë¶„.** microgptëŠ” ìˆœìˆ˜ Pythonì˜ ìŠ¤ì¹¼ë¼ `Value` ê°ì²´ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œì€ í…ì„œë¥¼ ì‚¬ìš©í•˜ê³  GPU/TPUì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ìˆ˜í•™ì€ ë™ì¼í•˜ê³ , ë§ì€ ìŠ¤ì¹¼ë¼ê°€ ë³‘ë ¬ë¡œ ì²˜ë¦¬ë˜ëŠ” ê²ƒì— í•´ë‹¹í•©ë‹ˆë‹¤.

**ì•„í‚¤í…ì²˜.** microgptëŠ” 4,192ê°œì˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. GPT-4ê¸‰ ëª¨ë¸ì€ ìˆ˜ì²œì–µ ê°œì…ë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ë§¤ìš° ë¹„ìŠ·í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì‹ ê²½ë§ì´ë©°, í›¨ì”¬ ë„“ê³ (ì„ë² ë”© ì°¨ì› 10,000+) ê¹ŠìŠµë‹ˆë‹¤(100+ ë ˆì´ì–´). í˜„ëŒ€ LLMì€ RoPE, GQA, ê²Œì´íŠ¸ ì„ í˜• í™œì„±í™”, MoE ë ˆì´ì–´ ë“± ëª‡ ê°€ì§€ ë” ë§ì€ ë ˆê³  ë¸”ë¡ì„ í†µí•©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì”ì°¨ ìŠ¤íŠ¸ë¦¼ ìœ„ì— ì–´í…ì…˜(í†µì‹ )ê³¼ MLP(ê³„ì‚°)ê°€ ë²ˆê°ˆì•„ ë°°ì¹˜ë˜ëŠ” í•µì‹¬ êµ¬ì¡°ëŠ” ì˜ ë³´ì¡´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**í•™ìŠµ.** ìŠ¤í…ë‹¹ í•˜ë‚˜ì˜ ë¬¸ì„œ ëŒ€ì‹ , í”„ë¡œë•ì…˜ í•™ìŠµì€ ëŒ€ê·œëª¨ ë°°ì¹˜, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì , í˜¼í•© ì •ë°€ë„, ì‹ ì¤‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í”„ë¡ í‹°ì–´ ëª¨ë¸ í•™ìŠµì—ëŠ” ìˆ˜ì²œ ëŒ€ì˜ GPUê°€ ëª‡ ë‹¬ê°„ ì‹¤í–‰ë©ë‹ˆë‹¤.

**í›„ì²˜ë¦¬(Post-training).** í•™ìŠµì—ì„œ ë‚˜ì˜¨ ê¸°ë³¸ ëª¨ë¸("ì‚¬ì „í•™ìŠµ" ëª¨ë¸)ì€ ë¬¸ì„œ ì™„ì„±ê¸°ì´ì§€ ì±—ë´‡ì´ ì•„ë‹™ë‹ˆë‹¤. ChatGPTë¡œ ë§Œë“œëŠ” ê²ƒì€ ë‘ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì²«ì§¸, SFT(ì§€ë„ ë¯¸ì„¸ì¡°ì •): ë¬¸ì„œë¥¼ íë ˆì´ì…˜ëœ ëŒ€í™”ë¡œ êµì²´í•˜ê³  ê³„ì† í•™ìŠµí•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œ ë³€í•˜ëŠ” ê²ƒì€ ì—†ìŠµë‹ˆë‹¤. ë‘˜ì§¸, RL(ê°•í™” í•™ìŠµ): ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ì ìˆ˜ê°€ ë§¤ê²¨ì§€ë©°(ì¸ê°„, "ì‹¬íŒ" ëª¨ë¸, ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´), ëª¨ë¸ì´ ê·¸ í”¼ë“œë°±ì—ì„œ í•™ìŠµí•©ë‹ˆë‹¤.

**ì¶”ë¡ .** ìˆ˜ë°±ë§Œ ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ì„ ì„œë¹™í•˜ë ¤ë©´ ìì²´ ì—”ì§€ë‹ˆì–´ë§ ìŠ¤íƒì´ í•„ìš”í•©ë‹ˆë‹¤: ìš”ì²­ ë°°ì¹­, KV ìºì‹œ ê´€ë¦¬ ë° í˜ì´ì§•(vLLM ë“±), ì†ë„ë¥¼ ìœ„í•œ íˆ¬ê¸°ì  ë””ì½”ë”©, ë©”ëª¨ë¦¬ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì–‘ìí™”(float16 ëŒ€ì‹  int8/int4), ì—¬ëŸ¬ GPUì— ëª¨ë¸ ë¶„ì‚°. ê·¼ë³¸ì ìœ¼ë¡œëŠ” ì—¬ì „íˆ ì‹œí€€ìŠ¤ì˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ì§€ë§Œ ë” ë¹ ë¥´ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ë§ì€ ì—”ì§€ë‹ˆì–´ë§ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

ì´ ëª¨ë“  ê²ƒì´ ì¤‘ìš”í•œ ì—”ì§€ë‹ˆì–´ë§ ë° ì—°êµ¬ ê¸°ì—¬ì´ì§€ë§Œ, microgptë¥¼ ì´í•´í•œë‹¤ë©´ ì•Œê³ ë¦¬ì¦˜ì  ë³¸ì§ˆì„ ì´í•´í•œ ê²ƒì…ë‹ˆë‹¤.

## FAQ

**ëª¨ë¸ì´ ë¬´ì–¸ê°€ë¥¼ "ì´í•´"í•˜ë‚˜ìš”?** ì² í•™ì  ì§ˆë¬¸ì´ì§€ë§Œ, ë©”ì»¤ë‹ˆì¦˜ì ìœ¼ë¡œëŠ”: ë§ˆë²•ì€ ì¼ì–´ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ë‹¤ìŒ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¡œ ë§¤í•‘í•˜ëŠ” í° ìˆ˜í•™ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ê²ƒì´ "ì´í•´"ë¥¼ êµ¬ì„±í•˜ëŠ”ì§€ëŠ” ì—¬ëŸ¬ë¶„ì—ê²Œ ë‹¬ë ¤ ìˆì§€ë§Œ, ë©”ì»¤ë‹ˆì¦˜ì€ ìœ„ì˜ 200ì¤„ì— ì™„ì „íˆ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.

**ì™œ ì‘ë™í•˜ë‚˜ìš”?** ëª¨ë¸ì€ ìˆ˜ì²œ ê°œì˜ ì¡°ì ˆ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆê³ , ì˜µí‹°ë§ˆì´ì €ê°€ ê° ìŠ¤í…ë§ˆë‹¤ ì¡°ê¸ˆì”© ì†ì‹¤ì„ ì¤„ì´ë„ë¡ ë°€ì–´ì¤ë‹ˆë‹¤. ë§ì€ ìŠ¤í…ì— ê±¸ì³ íŒŒë¼ë¯¸í„°ëŠ” ë°ì´í„°ì˜ í†µê³„ì  ê·œì¹™ì„±ì„ í¬ì°©í•˜ëŠ” ê°’ìœ¼ë¡œ ì •ì°©í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ëª…ì‹œì ì¸ ê·œì¹™ì„ ë°°ìš°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê·¸ê²ƒë“¤ì„ ë°˜ì˜í•˜ëŠ” í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

**ChatGPTì™€ ì–´ë–¤ ê´€ë ¨ì´ ìˆë‚˜ìš”?** ChatGPTëŠ” ì´ ë™ì¼í•œ í•µì‹¬ ë£¨í”„(ë‹¤ìŒ í† í° ì˜ˆì¸¡, ìƒ˜í”Œë§, ë°˜ë³µ)ê°€ ì—„ì²­ë‚˜ê²Œ í™•ì¥ë˜ê³ , ëŒ€í™”í˜•ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ í›„ì²˜ë¦¬ê°€ ì¶”ê°€ëœ ê²ƒì…ë‹ˆë‹¤. ëŒ€í™”í•  ë•Œ, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ì—¬ëŸ¬ë¶„ì˜ ë©”ì‹œì§€, ëª¨ë¸ì˜ ì‘ë‹µì€ ëª¨ë‘ ì‹œí€€ìŠ¤ì˜ í† í°ì¼ ë¿ì…ë‹ˆë‹¤.

**"í™˜ê°"ì´ ë­”ê°€ìš”?** ëª¨ë¸ì€ í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤. ì§„ì‹¤ì´ë¼ëŠ” ê°œë…ì´ ì—†ê³ , í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í†µê³„ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ ì‹œí€€ìŠ¤ë§Œ ì••ë‹ˆë‹¤. microgptê°€ "karia" ê°™ì€ ì´ë¦„ì„ "í™˜ê°"í•˜ëŠ” ê²ƒì€ ChatGPTê°€ ê±°ì§“ ì‚¬ì‹¤ì„ ìì‹  ìˆê²Œ ë§í•˜ëŠ” ê²ƒê³¼ ê°™ì€ í˜„ìƒì…ë‹ˆë‹¤.

**ì™œ ì´ë ‡ê²Œ ëŠë¦°ê°€ìš”?** microgptëŠ” ìˆœìˆ˜ Pythonì—ì„œ í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. GPUì—ì„œ ê°™ì€ ìˆ˜í•™ì€ ìˆ˜ë°±ë§Œ ê°œì˜ ìŠ¤ì¹¼ë¼ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ëª‡ ìë¦¿ìˆ˜ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë©ë‹ˆë‹¤.

**ë” ì¢‹ì€ ì´ë¦„ì„ ìƒì„±í•  ìˆ˜ ìˆë‚˜ìš”?** ë„¤. ë” ì˜¤ë˜ í•™ìŠµí•˜ê±°ë‚˜(`num_steps` ì¦ê°€), ëª¨ë¸ì„ ë” í¬ê²Œ ë§Œë“¤ê±°ë‚˜(`n_embd`, `n_layer`, `n_head`), ë” í° ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì´ê²ƒì€ ëŒ€ê·œëª¨ì—ì„œë„ ì¤‘ìš”í•œ ë™ì¼í•œ ì¡°ì ˆ ìš”ì†Œì…ë‹ˆë‹¤.

**ë°ì´í„°ì…‹ì„ ë°”ê¾¸ë©´?** ëª¨ë¸ì€ ë°ì´í„°ì— ìˆëŠ” ì–´ë–¤ íŒ¨í„´ì´ë“  í•™ìŠµí•©ë‹ˆë‹¤. ë„ì‹œ ì´ë¦„, í¬ì¼“ëª¬ ì´ë¦„, ì˜ì–´ ë‹¨ì–´, ë˜ëŠ” ì§§ì€ ì‹œë¡œ êµì²´í•˜ë©´ ëª¨ë¸ì€ ê·¸ê²ƒë“¤ì„ ìƒì„±í•˜ëŠ” ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

---

> ì›ë¬¸: [microgpt - Andrej Karpathy](https://karpathy.github.io/2026/02/12/microgpt/)
